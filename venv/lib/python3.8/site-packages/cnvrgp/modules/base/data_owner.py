import os
import re
import time
import yaml
import shutil

from cnvrgp.proxy import HTTP
from cnvrgp.data import FileCompare
from cnvrgp.data import FileUploader
from cnvrgp.data import FileDownloader
from cnvrgp.data import LocalFileDeleter
from cnvrgp.config import error_messages
from cnvrgp.data import RemoteFileDeleter
from cnvrgp.utils.url_utils import urljoin
from cnvrgp.data import ArtifactsDownloader
from cnvrgp.modules.file import File
from cnvrgp.errors import CnvrgFileError, CnvrgError
from cnvrgp.utils.api_list_generator import api_list_generator
from cnvrgp.modules.base.dynamic_attributes import DynamicAttributes
from cnvrgp.utils.storage_utils import get_files_and_dirs_recursive, create_dir_if_not_exists, filter_by_mimetypes


class DataOwner(DynamicAttributes):
    TMP_FOLDER_NAME = ".tmp"

    def __init__(self):
        # Data attributes
        self._working_dir = None
        self._storage_meta = None
        self._active_commit = None
        self.local_commit = None
        self.query = None

    @property
    def storage_meta(self):
        """
        Retrieve the storage meta of the data owner
        @return: storage meta in a dict
        """
        if self._storage_meta is None:
            response = self._proxy.call_api(
                route=urljoin(self._route, "storage_client"),
                http_method=HTTP.GET
            )
            storage_meta = response.meta["storage"]
            self._storage_meta = storage_meta

        return self._storage_meta

    @property
    def current_commit(self):
        """
        Returns the current commit for the data owner
        @return: string denoting current/active commit
        """
        return self._active_commit or self.last_commit

    @current_commit.setter
    def current_commit(self, commit_sha1):
        self._active_commit = commit_sha1

    @property
    def working_dir(self):
        """
        Returns the local working dir for the data owner
        @return: string denoting the path to the working dir
        """
        if self._working_dir:
            return self._working_dir
        elif os.environ.get("CNVRG_DATADIR"):
            return os.path.join(os.environ.get("CNVRG_DATADIR"), self.slug)
        else:
            return os.curdir

    @working_dir.setter
    def working_dir(self, working_dir):
        self._working_dir = working_dir

    def _start_commit(self, message="", source="sdk", parent_sha1=None, blank=False, force=False, job_slug=None):
        """
        Starts a new commit
        @param message: Commit message string (Optional)
        @param source: Source of the commit string (Optional)
        @param parent_sha1: Commit sha1 of the parent of the new commit
        @param blank: Start from a blank state or from a previous commit
        @param force: Start from a blank state or from a previous commit
        @param job_slug: Job that this commit related to
        @return: Dict containing commit data
        """
        data = {
            "force": force,
            "blank": blank,
            "job_slug": job_slug,
            "parent_sha1": parent_sha1,
            "source": source,
            "message": message
        }
        response = self._proxy.call_api(
            route=urljoin(self._route, "commits"),
            http_method=HTTP.POST,
            payload=data
        )

        self._active_commit = response.attributes["sha1"]

        return response.attributes

    def _end_commit(self, tag_images=False):
        """
        End the commit after uploading/deleting files from the dataset.
        @param tag_images: Will cause images in this commit to be tagged so they can be
            displayed in specific places on front
        @return: None
        """
        self._proxy.call_api(
            route=urljoin(self._route, "commits", self.current_commit, "end_commit"),
            http_method=HTTP.PUT,
            payload={'tag_images': tag_images, 'workflow_slug': self.slug}
        )

    def put_files(
        self,
        paths,
        pattern="*",
        message="",
        job_slug=None,
        blank=False,
        override=False,
        force=False,
        upload=False,
        tag_images=False
    ):
        """
        Uploads the files and folders given.
        If a folder is given all the relevant files in that folder (that confirms to the regex) will be uploaded.
        @param paths: List of file/folder paths
        @param pattern: String defining the filename pattern
        @param message: String defining the commit message
        @param job_slug: Slug of a job to upload the files to
        @param blank: Start from a blank state or from a previous commit
        @param override: Boolean stating whether or not we should re-upload even if the file already exists
        @param force: Boolean stating whether or not we the new commit should copy files from parent
        @param upload: Boolean gives info if the put files comes from upload or not
        @param tag_images: Boolean indicating if we want to allow only images to be uploaded. will be used by exp.log_image
        @return: None
        """
        fullpaths = []
        last_commit = None
        filters = []

        if tag_images:
            filters = ['image']

        for path in paths:
            is_dir = os.path.isdir(path)
            is_file = os.path.isfile(path)

            if is_dir:
                fullpaths += get_files_and_dirs_recursive(root_dir=path, regex=pattern, not_tmp=True, filters=filters)
            elif is_file:
                if filter_by_mimetypes(filters, path):
                    fullpaths.append(path)
                else:
                    print("{} does not match any filter skipping it".format(path))
            else:
                # TODO: different handling?
                print("{} does not exists skipping it".format(path))

        unique_fullpaths = list(set(fullpaths) - {"./", ".", "/", "//"})

        if not unique_fullpaths and not upload:
            # TODO: different handling?
            return

        if job_slug is not None:
            last_commit = self.last_commit or self.start_commit

        new_unique_fullpaths = []
        tmp_folder = "{}/{}".format(self.working_dir, DataOwner.TMP_FOLDER_NAME)
        for up in unique_fullpaths:
            deleted_file = "{}/{}.deleted".format(tmp_folder, up)

            if not os.path.isfile(deleted_file):
                new_unique_fullpaths.append(up)

        self._start_commit(message=message, blank=blank, parent_sha1=last_commit, job_slug=job_slug, force=force)

        uploader = FileUploader(self, new_unique_fullpaths, override=override)
        while uploader.in_progress:
            time.sleep(0.1)

        if upload:
            deleter = RemoteFileDeleter(self)
            while deleter.in_progress:
                time.sleep(0.1)

        self._end_commit(tag_images=tag_images)

    def clone(self):
        """
        Clones the remote project / dataset
        @return: None
        """
        # TODO: Need to consider working_dir if exist
        if not os.path.exists(self.slug):
            os.makedirs(self.slug)

        old_working_dir = self.working_dir
        self.working_dir = "{}/{}".format(old_working_dir, self.slug)

        downloader = FileDownloader(self)
        while downloader.in_progress:
            time.sleep(1)

        dot_cnvrg_path = "{}/{}".format(self.working_dir, ".cnvrg")
        if not os.path.exists(dot_cnvrg_path):
            os.makedirs(dot_cnvrg_path)

        with open("{}/{}".format(dot_cnvrg_path, "config.yaml"), 'w') as config:
            yaml.dump(
                {
                    "project_slug": self.slug,
                    "owner": self._context.organization,
                    "git": False,
                    "commit_sha1": self.current_commit
                },
                config
            )

        self.working_dir = old_working_dir

    def _clone_artifact(self, start_commit=None):
        """
        Clones the remote project / dataset
        @return: None
        """
        # TODO: Need to consider working_dir if exist
        if not start_commit:
            raise AttributeError("start commit must be sent if you send artifacts_only")

        return ArtifactsDownloader(self, base_commit_sha1=start_commit, commit_sha1=self.current_commit)

    def upload(self, sync=False, job_slug=None):
        """
        Uploads files to remote project / dataset
        @param sync: Boolean gives info if the put files comes from sync or not
        @return: None
        """
        if not sync:
            try:
                with open('./.cnvrg/config.yaml') as config:
                    config_data = yaml.load(config, Loader=yaml.FullLoader)
                    self.local_commit = config_data["commit_sha1"]
            except FileNotFoundError:
                raise CnvrgFileError(error_messages.CONFIG_YAML_NOT_FOUND)

        self.put_files(['.'], upload=True, job_slug=job_slug)

        if sync:
            self.move_files_from_tmp()

        self.update_config_file()

    def download(self, sync=False):
        """
        Download files from remote project / dataset
        @param sync: Boolean gives info if the put files comes from sync or not
        @return: None
        """
        try:
            with open('./.cnvrg/config.yaml') as config:
                config_data = yaml.load(config, Loader=yaml.FullLoader)
                self.local_commit = config_data["commit_sha1"]
        except FileNotFoundError:
            raise CnvrgFileError(error_messages.CONFIG_YAML_NOT_FOUND)

        os.mkdir(DataOwner.TMP_FOLDER_NAME)
        old_working_dir = self.working_dir
        self.working_dir = "{}/{}".format(self.working_dir, DataOwner.TMP_FOLDER_NAME)

        downloader = ArtifactsDownloader(self, base_commit_sha1=self.local_commit, commit_sha1=self.current_commit)
        while downloader.in_progress:
            time.sleep(1)

        self.working_dir = old_working_dir

        deleter = LocalFileDeleter(self)
        while deleter.in_progress:
            time.sleep(1)

        if not sync:
            self.move_files_from_tmp()
            self.update_config_file()

    def sync_local(self, job_slug=None):
        """
        Sync local project / dataset to remote
        @param job_slug: Slug of a job to upload the files to
        @return: None
        """
        try:
            self.download(sync=True)
            self.upload(sync=True, job_slug=job_slug)
        finally:
            # make sure the tmp folder used by download and upload will be deleted
            if os.path.exists(DataOwner.TMP_FOLDER_NAME):
                shutil.rmtree(DataOwner.TMP_FOLDER_NAME)

    def list_files(self, commit_sha1=None, query=None, query_raw=None, sort="-id"):
        """
        List all queries in a specific query
        @param commit_sha1: Sha1 of the commit files list
        @param query: Query slug to list files from
        @param query_raw: Raw query to list files (e.g. {color: yellow})
        @param sort: key to sort the list by (-key -> DESC | key -> ASC)
        @raise: HttpError
        @return: Generator that yields query objects
        """
        if query and query_raw:
            raise CnvrgError(error_messages.QUERY_LIST_FAULTY_PARAMS.format("query and query_raw"))

        if query and commit_sha1:
            raise CnvrgError(error_messages.QUERY_LIST_FAULTY_PARAMS.format("query and commit_sha1"))

        if not commit_sha1:
            commit_sha1 = self.current_commit

        return api_list_generator(
            context=self._context,
            route=urljoin(self._route, "commits", commit_sha1, "files"),
            object=File,
            sort=sort,
            identifier="fullpath",
            pagination_type="offset",
            data={
                "query": query,
                "query_raw": query_raw
            }
        )

    def update_config_file(self):
        """
        Update the config file of the data_owner with latest commit
        @return: None
        """
        with open("./.cnvrg/config.yaml", 'w') as config:
            yaml.dump(
                {
                    "project_slug": self.slug,
                    "owner": self._context.organization,
                    "git": False,
                    "commit_sha1": self.current_commit
                },
                config
            )

    def move_files_from_tmp(self):
        """
        Takes files from .tmp folder and complete the sync/download/upload operation
        @return: None
        """
        fullpaths = get_files_and_dirs_recursive(root_dir='.', not_tmp=True)
        unique_fullpaths = list(set(fullpaths) - {"./", ".", "/", "//"})

        if not unique_fullpaths:
            # TODO: different handling?
            return

        compare = FileCompare(self, unique_fullpaths)
        while compare.in_progress:
            time.sleep(1)

        tmp_folder = "{}/{}".format(self.working_dir, DataOwner.TMP_FOLDER_NAME)
        downloaded_files = get_files_and_dirs_recursive(root_dir=tmp_folder, only_files=True)
        for df in downloaded_files:
            if df == ".tmp/" or df == ".tmp/.cnvrgignore":
                continue

            if df.endswith('.deleted'):
                original_file = re.sub(DataOwner.TMP_FOLDER_NAME, self.working_dir, df)
                original_file = re.sub(".deleted", "", original_file)
                os.remove(original_file)
                continue

            new_path = re.sub(DataOwner.TMP_FOLDER_NAME, self.working_dir, df)
            create_dir_if_not_exists(new_path)
            os.rename(df, new_path)

        shutil.rmtree(tmp_folder)
